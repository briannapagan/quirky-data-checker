{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "from varinfo import VarInfoFromDmr\n",
    "from cmr import CollectionQuery, GranuleQuery, ToolQuery, ServiceQuery, VariableQuery\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provider GES_DISC has 1791 to check\n"
     ]
    }
   ],
   "source": [
    "provider = 'GES_DISC'\n",
    "api = CollectionQuery()\n",
    "collections = api.provider(provider).get_all()\n",
    "\n",
    "print('provider ' + provider + ' has ' + str(np.shape(collections)[0]) + ' to check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sad that because GES DISC didn't leverage opendap flags we have to use this\n",
    "# silly way to identify those collections :( )\n",
    "\n",
    "# should we add filter for just checking level 3/4 products? \n",
    "# for now will summarize by this info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provider GES_DISC has 1305 opendap collections to check\n"
     ]
    }
   ],
   "source": [
    "opendap_base_urls = []\n",
    "short_names = []\n",
    "versions = []\n",
    "processing_levels = []\n",
    "native_ids = []\n",
    "\n",
    "for collection in collections:\n",
    "    for element in collection['links']:\n",
    "        if element['rel'] == 'http://esipfed.org/ns/fedsearch/1.1/service#':\n",
    "            # this should return default url so can be on prem or cloud depending on granule\n",
    "            opendap_base_urls.append(element['href'])\n",
    "            processing_levels.append(collection['processing_level_id'])\n",
    "            versions.append(collection['version_id'])\n",
    "            short_names.append(collection['short_name'])\n",
    "            native_ids.append(collection['id'])\n",
    "            break\n",
    "\n",
    "print('provider ' + provider + ' has ' + str(np.shape(short_names)[0]) + ' opendap collections to check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get granule for each collection, try to open with xarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = GranuleQuery()\n",
    "granule = api.short_name(short_names[0]).version(versions[0]).get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if dimension variables were read as dimension variables with xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dmr(opendap_url: str) -> str:\n",
    "    ''' Given an OPeNDAP url use the requests library to save the\n",
    "        `.dmr` file locally for `earthdata-varinfo` \n",
    "    '''\n",
    "    out_path = os.path.join(os.getcwd(), 'dmr_data')\n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "    out_filename = os.path.join(out_path, Path(opendap_url).stem + '.dmr')\n",
    "    dmr_opendap_url = opendap_url + '.dmr.xml'\n",
    "    response = requests.get(url=dmr_opendap_url)\n",
    "    if response.ok:\n",
    "        with open(out_filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    return out_filename\n",
    "\n",
    "\n",
    "def is_spatial_temporal_dimension(out_filename: str,\n",
    "                                  var_name: str) -> str:\n",
    "    ''' Use `VarInfoFromDmr` to check if a variable\n",
    "        is a spatial temporal variable\n",
    "    '''\n",
    "    varinfo_dmr = VarInfoFromDmr(out_filename)\n",
    "    if (varinfo_dmr.get_variable('/' + var_name).is_temporal()\n",
    "        or varinfo_dmr.get_variable('/' + var_name).is_geographic()\n",
    "        or varinfo_dmr.get_variable('/' + var_name).is_projection_x_or_y()):\n",
    "        return 'Dimension variable not mapped correctly'\n",
    "    else:\n",
    "        return 'n/a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_out = []\n",
    "short_names_out = []\n",
    "versions_out = []\n",
    "opendap_urls_out = []\n",
    "processing_levels_out = []\n",
    "\n",
    "for i in range(0,np.shape(short_names)[0]):\n",
    "    print(i)\n",
    "    granule = api.short_name(short_names[i]).version(versions[i]).get(1) \n",
    "    for element in granule[0]['links']:\n",
    "        if 'title' in element:\n",
    "            if element['title'] == 'The OPENDAP location for the granule. (GET DATA : OPENDAP DATA)':\n",
    "                # this should return default url so can be on prem or cloud depending on granule\n",
    "                opendap_url = (element['href'])\n",
    "                opendap_urls_out.append(opendap_url)\n",
    "                try:\n",
    "                    dataset = xr.open_dataset(opendap_url)\n",
    "                    message_out.append('success')\n",
    "                    short_names_out.append(short_names[i])\n",
    "                    versions_out.append(versions[i])\n",
    "                    processing_levels_out.append(processing_levels[i])\n",
    "                    \n",
    "                    # Check if data variables were determined to be dimensions\n",
    "                    dmr_filename = get_dmr(opendap_url)\n",
    "                    data_vars = list(dataset.data_vars)\n",
    "                    for var_name in data_vars:\n",
    "                        print(var_name, is_spatial_temporal_dimension(dmr_filename, var_name))\n",
    "                except Exception as e:\n",
    "                    message_out.append(e)\n",
    "                    short_names_out.append(short_names[i])\n",
    "                    versions_out.append(versions[i])\n",
    "                    processing_levels_out.append(processing_levels[i])\n",
    "\n",
    "        #else:\n",
    "            #message_out.append('no opendap url found')\n",
    "    df_out = pd.DataFrame({'short_name':short_names_out,'version':versions_out,'processing_level':processing_levels_out,'message':message_out,'opendap_url':opendap_urls_out})\n",
    "    df_out.to_csv('results_' + provider +  '.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
